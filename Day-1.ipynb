{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeoedc1KEvYd85Kbk9wefr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avinashdixitt/5-Day-Gen-AI-Intensive-Course-with-Google-Learn-Guide/blob/main/Day-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLmYYejwQhdt",
        "outputId": "a1aae199-b2da-4f20-d5fa-226959e5dd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q \"google-genai==1.7.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "tgTaJpusQ1qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "vOrVTXfjRIEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GNFUuyrCSc3I",
        "outputId": "70663b33-ad4f-4f7c-b2b3-d50394481a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyBJDyjHLP-8SGjD4UkTQ18HYPMg0OJTUt8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key='AIzaSyBJDyjHLP-8SGjD4UkTQ18HYPMg0OJTUt8')\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"Explain AI to me like I'm a kid.\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtJH5dvZSuY0",
        "outputId": "6ccdd430-fb36-40ee-ee8e-e5b929ea83fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a really, really smart robot puppy! That robot puppy can learn new things, just like a real puppy.\n",
            "\n",
            "AI is like giving that robot puppy a super brain that helps it learn even faster! \n",
            "\n",
            "Instead of just knowing how to fetch, AI helps it learn:\n",
            "\n",
            "*   **Recognize things:** \"That's a ball!\", \"That's a cat!\", \"That's YOU!\" (It learns by seeing lots and lots of pictures of balls, cats, and you!)\n",
            "*   **Understand what you say:** If you say \"Sit\", it knows to put its bottom on the ground. (It learns by hearing lots of people say \"Sit\" while someone is doing it!)\n",
            "*   **Solve problems:** If there's a toy stuck under the couch, it can figure out how to get it out. (It learns by trying different things and seeing what works!)\n",
            "*   **Even talk to you:** It can learn to answer your questions and have a conversation, like a really smart chatty robot puppy!\n",
            "\n",
            "So, AI is basically a way to make computers and robots really smart, so they can learn and do things that usually only people can do. It's like magic, but it's actually clever computer programming!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "j-m7ZgjwS4rT",
        "outputId": "805ac0aa-0385-4029-f728-dbd7050ace90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine you have a really, really smart robot puppy! That robot puppy can learn new things, just like a real puppy.\n\nAI is like giving that robot puppy a super brain that helps it learn even faster! \n\nInstead of just knowing how to fetch, AI helps it learn:\n\n*   **Recognize things:** \"That's a ball!\", \"That's a cat!\", \"That's YOU!\" (It learns by seeing lots and lots of pictures of balls, cats, and you!)\n*   **Understand what you say:** If you say \"Sit\", it knows to put its bottom on the ground. (It learns by hearing lots of people say \"Sit\" while someone is doing it!)\n*   **Solve problems:** If there's a toy stuck under the couch, it can figure out how to get it out. (It learns by trying different things and seeing what works!)\n*   **Even talk to you:** It can learn to answer your questions and have a conversation, like a really smart chatty robot puppy!\n\nSo, AI is basically a way to make computers and robots really smart, so they can learn and do things that usually only people can do. It's like magic, but it's actually clever computer programming!\n"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Zlork.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgUrcE_8S_y9",
        "outputId": "5a28843b-a49d-4353-f4fc-0e4314fc5c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, Zlork! It's a pleasure to make your acquaintance. Is there anything I can help you with today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Can you tell me something interesting about dinosaurs?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZje-WIdTTOV",
        "outputId": "5ea4c8d7-6b05-4390-d600-dd87a286a885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's something interesting about dinosaurs that might surprise you:\n",
            "\n",
            "**Some dinosaurs, like the Microraptor, likely had iridescent feathers!**\n",
            "\n",
            "We often picture dinosaurs as scaly, but evidence suggests that many, especially theropods (the group that includes Tyrannosaurus Rex and birds), were covered in feathers. Even more fascinating, fossilized melanosomes (pigment-containing structures) have been found in Microraptor feathers. Analysis of these melanosomes indicates that Microraptor's feathers were likely not just black, but **glossy and iridescent**, much like the feathers of a modern-day raven or starling!\n",
            "\n",
            "This means that these small, four-winged dinosaurs (Microraptor had feathers on its arms and legs) would have shimmered and gleamed in the sunlight. Pretty cool, right?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Can you GIVE ME IMAGE?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV5h3R_oTfva",
        "outputId": "eec2dccb-8eea-493b-b0fb-80ffb71e7f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I understand you're asking for an image. Unfortunately, as a text-based AI, I am not capable of generating or displaying images. I can only provide information in text format.\n",
            "\n",
            "Is there anything else I can help you with related to dinosaurs, or perhaps another topic? I can provide facts, stories, or answer questions to the best of my ability.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBaLEzTBT8xs",
        "outputId": "f1e9c97f-1a15-41a7-e85a-d4607952b02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.0-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk23LQTWUImI",
        "outputId": "3963a747-c350-47bd-8ed6-c26d71fc3d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens', 'createCachedContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=short_config,\n",
        "    contents='Write a 1000 word essay on the importance of olives in modern society.')\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nuz-G4OUPRm",
        "outputId": "fe47d2cc-1a9b-4357-8777-9892f115409f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Humble Olive: A Cornerstone of Modern Society\n",
            "\n",
            "The olive, a small fruit brimming with history and flavor, often goes unnoticed amidst the dazzling array of modern food choices. Yet, to dismiss the olive as merely a Mediterranean appetizer is to ignore its profound and multifaceted importance in modern society. From its crucial role in global cuisine and the health benefits derived from its oil, to its environmental sustainability and contribution to diverse economies, the olive has quietly but profoundly shaped our world and continues to be an indispensable element of contemporary life.\n",
            "\n",
            "One of the most significant contributions of the olive lies in its culinary versatility. For millennia, olives and their oil have been staples of the Mediterranean diet, a dietary pattern lauded for its health benefits and embraced globally. Olives themselves, cured and preserved in myriad ways, offer a burst of flavor that elevates dishes from simple salads to elaborate tapas. The briny tang of Kalamata olives, the earthy notes of Spanish Manzanillas, and the pungent intensity of Nicoise olives represent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=short_config,\n",
        "    contents='Write a short poem on the importance of olives in modern society.')\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CShUHUYEUYza",
        "outputId": "779db00e-104f-4f06-f217-b1b2e25143e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From sun-soaked groves, a humble grace,\n",
            "The olive thrives in time and space.\n",
            "In salads bright, a briny treat,\n",
            "On pizza pies, a flavor sweet.\n",
            "\n",
            "As oil it flows, a golden stream,\n",
            "Nourishing bodies, a healthy dream.\n",
            "From tapenades to artisan bread,\n",
            "A vital role, expertly spread.\n",
            "\n",
            "So raise a glass, to this small fruit,\n",
            "Whose simple presence bears such loot.\n",
            "The olive reigns, a modern key,\n",
            "To taste and health, for you and me.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=high_temp_config,\n",
        "      contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snuGn9sbUegu",
        "outputId": "8345f29d-5eb0-46eb-b2e6-23c00df10b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turquoise\n",
            " -------------------------\n",
            "Magenta\n",
            " -------------------------\n",
            "Orange.\n",
            " -------------------------\n",
            "Orange\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=low_temp_config,\n",
        "      contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vghNYuWoUpWH",
        "outputId": "38521933-5f4d-4c0d-fd7b-55d111e32fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n",
            "Azure\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    # These are the default values for gemini-2.0-flash.\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=story_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mwQpbIMUukD",
        "outputId": "8eee5bb8-63fe-4edd-b1ae-2b0c7d4cab37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clementine, a tortoiseshell tabby of discerning taste and an even more discerning yawn, considered herself a creature of habit. Naps in sunbeams, chasing dust bunnies, the occasional disdainful glance at the neighborhood squirrels - these were the cornerstones of a well-lived feline existence. But today, a rebellious flicker ignited in her emerald eyes. Today, Clementine craved adventure.\n",
            "\n",
            "It started with the scent. A wild, unfamiliar aroma, a heady blend of damp earth, blooming honeysuckle, and something indefinably *other*, drifted in through the slightly ajar kitchen window. It beckoned, a siren song for a kitty soul yearning for something more than a bowl of salmon pate.\n",
            "\n",
            "She slipped out the window, a fluid ripple of fur and determination. The familiar brick patio was quickly left behind as she navigated the overgrown rose bushes, their thorns, normally a deterrent, now mere inconveniences. The scent grew stronger, leading her towards the untamed wilderness that bordered the manicured lawns of her suburban neighborhood.\n",
            "\n",
            "This was new territory. Towering ferns brushed against her whiskers, their fronds whispering secrets in a language she almost understood. Tiny woodland creatures, usually the target of her aloof indifference, scurried past, their eyes wide with a primal fear she'd never inspired before. Clementine felt a thrill, a jolt of wildness that resonated deep within her.\n",
            "\n",
            "Following the intoxicating scent, she found herself at the edge of a small stream, its water sparkling under the dappled sunlight. On the other side, a riot of wildflowers exploded in color, a scene so vibrant it made her usually unimpressed heart skip a beat. The scent, she realized, was strongest here.\n",
            "\n",
            "Hesitantly, she plunged a paw into the cool water. It was shocking, then refreshing. She braved the crossing, shivering slightly, her fur plastered to her body. On the other side, she shook herself like a tiny, furry earthquake and ventured into the floral paradise.\n",
            "\n",
            "Butterflies, their wings painted with intricate patterns, flitted around her head. Bees buzzed lazily, their bodies dusted with pollen. And there, nestled amongst the blossoms, was the source of the enchanting scent: a patch of wild honeysuckle, its creamy white flowers dripping with nectar.\n",
            "\n",
            "Clementine, forgetting her usual disdain for all things saccharine, cautiously lapped at the sweet liquid. The taste was intoxicating, a burst of sunshine and summer in her mouth. She spent a blissful hour exploring the wildflower meadow, chasing butterflies, swatting at bumblebees (albeit gently), and bathing in the fragrant air.\n",
            "\n",
            "As the sun began to dip below the horizon, painting the sky in hues of orange and purple, a familiar pang of hunger stirred in her belly. It was time to go home.\n",
            "\n",
            "The journey back was quicker, fueled by a newfound confidence. She crossed the stream with a more experienced air, her paws hardly hesitating. Emerging from the wilderness, she saw her familiar patio beckoning in the twilight.\n",
            "\n",
            "Back inside, curled up on her favorite cushion, the memory of the wildflower meadow lingered like a sweet dream. She was still Clementine, the discerning cat of habit. But now, she was also Clementine, the adventurer. And as she drifted off to sleep, she knew, with a certainty that purred in her chest, that this was only the beginning of her wilder life. Tomorrow, she would explore the shed. After a nap, of course.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_output_tokens=5,\n",
        ")\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYMtL3kDVCVq",
        "outputId": "b53e5fae-0f47-4f0c-f766-9d8cb51da769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ),\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP_9ShgyVLBG",
        "outputId": "ee72bbb5-81c1-4461-b820-9c94b426e658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = input(\"enter you order\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ),\n",
        "    contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpmFe3ABVZ1l",
        "outputId": "4960180c-ff87-418b-cf9f-87bdd17113c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter you order1 burger de do usme paneer aur namak aur masla sal dena thodi si pyaj bhi lahsun ho to vo bhi aur pizza mt dena\n",
            "```json\n",
            "{\n",
            "  \"type\": \"burger\",\n",
            "  \"ingredients\": [\"paneer\", \"namak\", \"masla\", \"pyaj\", \"lahsun\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ),\n",
        "    contents= input(\"enter you order\")\n",
        "\n",
        "print(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "CiCRZ18kWO0C",
        "outputId": "ba0f3475-6147-4218-c9e4-be72c5d8bb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-32-1042d34947f3>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-1042d34947f3>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    contents= input(\"enter you order\")\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ),\n",
        "    contents=\"aloo pyaj pani roti chhota\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeOy47isWqw2",
        "outputId": "1b277fa3-d194-4662-aa8b-977cdd129e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"chhota\",\n",
            "  \"ingredients\": [\"aloo\", \"pyaj\"],\n",
            "  \"type\": \"roti\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 5 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x193J2djXCu5",
        "outputId": "fe5ba5f6-315a-4cff-c75d-e7f1f3181cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "-mEh-62DXKWG",
        "outputId": "8b0397e3-617f-4f6e-ee36-25f1ffcbcf55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break this down:\n\n1. **When you were 4, your partner was 3 times your age:** That means your partner was 4 * 3 = 12 years old.\n\n2. **The age difference remains constant:** The difference in age between you and your partner will always be the same. So, your partner is 12 - 4 = 8 years older than you.\n\n3. **Now you are 20:** Since your partner is 8 years older than you, they are currently 20 + 8 = 28 years old.\n\n**Therefore, your partner is 28 years old.**\n"
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "id": "ggMG0BRiXRnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences=[\"\\nObservation\"],\n",
        "    system_instruction=model_instructions + example1 + example2,\n",
        ")\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_chat = client.chats.create(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=react_config,\n",
        ")\n",
        "\n",
        "resp = react_chat.send_message(question)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXMbIcS7XeG9",
        "outputId": "8049f55a-32f2-4fac-c3c0-9c7446abd9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the Transformers NLP paper and then identify the youngest author listed on the paper.\n",
            "\n",
            "Action 1\n",
            "<search>Transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccVmjQqOXkWa",
        "outputId": "804feca8-31c6-40ce-9d6d-0f61441c786a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I have the authors of the paper: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find their ages when the paper was published in 2017. I'll start with Ashish Vaswani.\n",
            "\n",
            "Action 1\n",
            "<search>Ashish Vaswani age</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        ")\n",
        "\n",
        "buf = io.StringIO()\n",
        "for chunk in response:\n",
        "    buf.write(chunk.text)\n",
        "    # Display the response as it is streamed\n",
        "    print(chunk.text, end='')\n",
        "\n",
        "# And then render the finished response as formatted markdown.\n",
        "clear_output()\n",
        "Markdown(buf.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "kUGJetuaXrrA",
        "outputId": "cd9b21bf-c6fe-4139-a994-60819f85aaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on publicly available information, the youngest author listed on the \"Attention is All You Need\" (Transformers) paper is **Aidan N. Gomez**.\n\nHere's why:\n\n* **Aidan N. Gomez** was a PhD student at the University of Toronto at the time of the paper's publication in 2017. PhD students are generally at an earlier stage in their careers and younger than researchers working at companies like Google Brain.\n\n* The other authors were primarily researchers at Google Brain (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Łukasz Kaiser, Illia Polosukhin). Researchers at Google Brain are typically post-doctoral or have significant experience, placing them at a later career stage and likely older than a PhD student.\n\nWhile we don't have exact birthdates for all authors to definitively confirm the absolute youngest, **Aidan N. Gomez being a PhD student at the time strongly suggests he was the youngest author on the paper.**"
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ),\n",
        "    contents=code_prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "MRFeLp2sX7Zj",
        "outputId": "337fa974-f335-4712-c7df-71f5fb2f240f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  \"\"\"\n  Calculate the factorial of a number.\n\n  Args:\n    n: An integer for which to calculate the factorial.\n\n  Returns:\n    The factorial of n.\n  \"\"\"\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
        ")\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Generate the first 14 odd prime numbers, then calculate their sum.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=config,\n",
        "    contents=code_exec_prompt)\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "  pprint(part.to_json_dict())\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-hLc1VnYCKF",
        "outputId": "759f5f1b-6300-4b36-e01f-ca1f24ec8e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"Okay, I can do that. First, I'll list the first 14 odd prime \"\n",
            "         'numbers. Remember that a prime number is a number greater than 1 '\n",
            "         'that has only two factors: 1 and itself.  Odd prime numbers are '\n",
            "         'prime numbers that are not divisible by 2.\\n'\n",
            "         '\\n'\n",
            "         'Here are the first 14 odd prime numbers: 3, 5, 7, 11, 13, 17, 19, '\n",
            "         '23, 29, 31, 37, 41, 43, 47\\n'\n",
            "         '\\n'\n",
            "         \"Now, I'll calculate their sum using a python code block:\\n\"\n",
            "         '\\n'}\n",
            "-----\n",
            "{'executable_code': {'code': 'numbers = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
            "                             '37, 41, 43, 47]\\n'\n",
            "                             'sum_of_numbers = sum(numbers)\\n'\n",
            "                             'print(sum_of_numbers)\\n',\n",
            "                     'language': 'PYTHON'}}\n",
            "-----\n",
            "{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': '326\\n'}}\n",
            "-----\n",
            "{'text': 'Therefore, the sum of the first 14 odd prime numbers is 326.\\n'}\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "    if part.text:\n",
        "        display(Markdown(part.text))\n",
        "    elif part.executable_code:\n",
        "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
        "    elif part.code_execution_result:\n",
        "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
        "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
        "\n",
        "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "gawuVK8sYCQV",
        "outputId": "181b256b-da77-4251-ef56-c9da4b0b2a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I can do that. First, I'll list the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two factors: 1 and itself.  Odd prime numbers are prime numbers that are not divisible by 2.\n\nHere are the first 14 odd prime numbers: 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47\n\nNow, I'll calculate their sum using a python code block:\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nnumbers = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_numbers = sum(numbers)\nprint(sum_of_numbers)\n\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\n326\n\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Therefore, the sum of the first 14 odd prime numbers is 326.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=explain_prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "gSB5KmmWYPR3",
        "outputId": "7d828112-a53e-4208-b871-c86def21217f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a shell script designed to enhance your command-line prompt with information about the current Git repository.  It's often called `bash-git-prompt` or similar.\n\nHere's a breakdown:\n\n*   **What it is:** It's a script you source (include) in your `~/.bashrc` or `~/.zshrc` file (or equivalent for your shell).  It defines functions and sets variables that customize how your prompt looks.\n\n*   **What it does:**\n\n    *   **Git Status:**  The core function is to display information about the Git repository you're currently in, directly in your prompt.  This includes:\n        *   The current branch name.\n        *   Whether there are staged, unstaged, or untracked files.\n        *   Whether your branch is ahead, behind, or diverged from the remote.\n        *   Whether there are merge conflicts.\n        *   Status of submodules\n        *   Other Git-related information.\n    *   **Customization:**  It allows you to customize the colors, symbols, and overall look of the Git information displayed in your prompt.  It does this through themes and configuration options.\n    *   **Performance:** Includes asynchronous fetching so that you can continue working while the prompt is fetching the latest git status.\n    *   **Virtual Environment Support:** It can optionally display the active Python virtual environment in your prompt.\n    *   **Last Command Status:** Can display an indicator based on whether the last command succeeded or failed.\n    *   **Cross-Shell Compatibility:** Attempts to work with both Bash and Zsh, although it has some conditional logic to handle differences between the two.\n\n*   **Why you'd use it:**\n\n    *   **Quick Git Status:**  You can instantly see the state of your Git repository without having to run `git status` manually. This saves time and keeps you informed.\n    *   **Improved Workflow:** By making Git information readily available, it can improve your Git workflow and reduce errors.\n    *   **Personalization:** You can tailor the prompt to match your preferences, making your terminal more visually appealing and informative.\n\nIn short, this script enhances your terminal prompt with real-time Git information, improving your workflow and providing a more informative command-line experience. To use it, you'd typically download the script, then add a `source` command to your shell's configuration file (e.g., `~/.bashrc` or `~/.zshrc`) to load the script when you open a new terminal. There are a number of options to configure its behavior as well.\n"
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}